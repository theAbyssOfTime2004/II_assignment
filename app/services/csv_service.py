from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime
import uuid
import json
import pandas as pd
from pathlib import Path
from llama_index.experimental.query_engine import PandasQueryEngine
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.core import Settings
from app.services.redis_service import redis_service
from app.core.config import OPENAI_API_KEY, OPENAI_MODEL_NAME
import logging
import requests
from io import StringIO

logger = logging.getLogger(__name__)

class CSVError(Exception):
    """Custom exception for CSV processing errors"""
    pass

class CSVService:
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    MAX_ROWS = 1_000_000  # 1 million rows
    ALLOWED_EXTENSIONS = {'.csv', '.txt'}
    
    def __init__(self):
        self.upload_dir = Path("uploads/csv")
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        
        # Configure llama-index to use gpt-4o-mini
        try:
            Settings.llm = LlamaOpenAI(
                api_key=OPENAI_API_KEY,
                model=OPENAI_MODEL_NAME,
                temperature=0.7
            )
            logger.info(f"LlamaIndex configured to use {OPENAI_MODEL_NAME}")
        except Exception as e:
            logger.error(f"Failed to configure LlamaIndex: {e}")

    async def process_csv_message(
        self,
        message: str,
        csv_data: Optional[bytes] = None,
        csv_url: Optional[str] = None,
        conversation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        conversation_id = conversation_id or str(uuid.uuid4())
        
        try:
            # **NEW: Check for conflicting inputs**
            if csv_data and csv_url:
                return {
                    "response": "âš ï¸ **Lá»—i:** KhÃ´ng thá»ƒ xá»­ lÃ½ Ä‘á»“ng thá»i cáº£ file upload vÃ  URL.\n\nðŸ’¡ **Gá»£i Ã½:** Chá»‰ chá»n má»™t trong hai:\n- Upload file CSV trá»±c tiáº¿p, HOáº¶C\n- Cung cáº¥p URL CSV",
                    "conversation_id": conversation_id
                }
        
            # Load or get DataFrame
            df = await self.get_or_load_dataframe(conversation_id, csv_data, csv_url)
            
            if df is None:
                return {
                    "response": "âš ï¸ **Lá»—i:** Vui lÃ²ng upload file CSV hoáº·c cung cáº¥p URL trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.\n\nðŸ“„ **Äá»‹nh dáº¡ng há»— trá»£:** .csv, .txt\nðŸ“ **KÃ­ch thÆ°á»›c tá»‘i Ä‘a:** 50MB",
                    "conversation_id": conversation_id
                }
            
            # Load history
            history = await self.get_history(conversation_id)
            
            # Add user message
            user_msg = {
                "role": "user",
                "content": message,
                "timestamp": datetime.now().isoformat()
            }
            history.append(user_msg)
            
            # Process query
            response_text, chart_data = await self.query_dataframe(df, message)
            
            # Add assistant message
            assistant_msg = {
                "role": "assistant",
                "content": response_text,
                "timestamp": datetime.now().isoformat()
            }
            history.append(assistant_msg)
            
            # Save history
            await self.save_history(conversation_id, history)
            
            return {
                "response": response_text,
                "conversation_id": conversation_id,
                "chart_data": chart_data
            }
            
        except CSVError as e:
            logger.error(f"CSV processing error: {e}")
            return {
                "response": f"âš ï¸ **Lá»—i CSV:** {str(e)}",
                "conversation_id": conversation_id
            }
        except Exception as e:
            logger.error(f"Unexpected error in CSV processing: {e}")
            return {
                "response": "âŒ **Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh.** Vui lÃ²ng thá»­ láº¡i hoáº·c upload file CSV khÃ¡c.",
                "conversation_id": conversation_id
            }

    async def get_or_load_dataframe(
        self,
        conversation_id: str,
        csv_data: Optional[bytes] = None,
        csv_url: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        csv_path = self.upload_dir / f"{conversation_id}.csv"
        
        # If CSV already exists for this conversation
        if csv_path.exists():
            try:
                return pd.read_csv(csv_path)
            except Exception as e:
                logger.error(f"Error reading existing CSV: {e}")
                raise CSVError("File CSV Ä‘Ã£ lÆ°u bá»‹ lá»—i. Vui lÃ²ng upload láº¡i.")
        
        # Load from bytes
        if csv_data:
            return await self._load_from_bytes(csv_data, csv_path)
        
        # Load from URL
        if csv_url:
            return await self._load_from_url(csv_url, csv_path)
        
        return None

    async def _load_from_bytes(self, csv_data: bytes, csv_path: Path) -> pd.DataFrame:
        """Load CSV from uploaded bytes with validation"""
        try:
            # Check file size
            if len(csv_data) > self.MAX_FILE_SIZE:
                raise CSVError(f"File quÃ¡ lá»›n (max {self.MAX_FILE_SIZE // (1024*1024)}MB). Vui lÃ²ng upload file nhá» hÆ¡n.")
            
            # Check if empty
            if len(csv_data) == 0:
                raise CSVError("File CSV trá»‘ng. Vui lÃ²ng upload file cÃ³ dá»¯ liá»‡u.")
            
            # Save to temp file
            with open(csv_path, "wb") as f:
                f.write(csv_data)
            
            # Try to read with multiple encodings
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            df = None
            last_error = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding)
                    break
                except UnicodeDecodeError as e:
                    last_error = e
                    continue
            
            if df is None:
                raise CSVError(f"KhÃ´ng thá»ƒ Ä‘á»c file CSV. Encoding khÃ´ng há»£p lá»‡: {last_error}")
            
            # Validate DataFrame
            return self._validate_dataframe(df, csv_path)
            
        except pd.errors.EmptyDataError:
            raise CSVError("File CSV khÃ´ng cÃ³ dá»¯ liá»‡u.")
        except pd.errors.ParserError as e:
            raise CSVError(f"Lá»—i parse CSV: {str(e)}. File cÃ³ thá»ƒ bá»‹ há»ng hoáº·c khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng.")
        except Exception as e:
            if csv_path.exists():
                csv_path.unlink()  # Clean up
            raise CSVError(f"Lá»—i khi Ä‘á»c file: {str(e)}")

    async def _load_from_url(self, csv_url: str, csv_path: Path) -> Optional[pd.DataFrame]:
        """Load CSV from URL with validation"""
        try:
            # Validate URL format
            if not csv_url.startswith(('http://', 'https://')):
                raise CSVError("URL khÃ´ng há»£p lá»‡. URL pháº£i báº¯t Ä‘áº§u báº±ng http:// hoáº·c https://")
            
            # **NEW: Auto-convert GitHub URLs to raw URLs**
            if 'github.com' in csv_url and '/blob/' in csv_url:
                csv_url = csv_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                logger.info(f"Converted GitHub URL to raw URL: {csv_url}")
            
            # Check if URL ends with .csv
            if not any(csv_url.lower().endswith(ext) for ext in ['.csv', '.txt']):
                raise CSVError("URL pháº£i káº¿t thÃºc báº±ng .csv hoáº·c .txt\n\nðŸ’¡ **VÃ­ dá»¥:** https://example.com/data.csv")
            # Try to read with timeout and size limit
            from io import StringIO

            logger.info(f"Fetching CSV from URL: {csv_url}")
            
            response = requests.get(csv_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '').lower()
            
            # **NEW: Better content-type validation with helpful error**
            if 'text/html' in content_type:
                logger.warning(f"Received HTML instead of CSV from: {csv_url}")
                error_msg = "âš ï¸ **URL tráº£ vá» trang HTML thay vÃ¬ file CSV.**\n\n"
                
                if 'github.com' in csv_url:
                    raw_url = csv_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                    error_msg += f"ðŸ’¡ **GitHub URL:** Sá»­ dá»¥ng URL raw thay vÃ¬ URL blob:\n\n"
                    error_msg += f"âŒ **Sai:** `{csv_url}`\n\n"
                    error_msg += f"âœ… **ÄÃºng:** `{raw_url}`\n\n"
                    error_msg += f"**Hoáº·c:** Click nÃºt 'Raw' trÃªn trang GitHub Ä‘á»ƒ láº¥y URL Ä‘Ãºng."
                else:
                    error_msg += "ðŸ’¡ **Gá»£i Ã½:**\n"
                    error_msg += "- Äáº£m báº£o URL trá» trá»±c tiáº¿p Ä‘áº¿n file CSV (khÃ´ng pháº£i trang web)\n"
                    error_msg += "- Thá»­ click chuá»™t pháº£i vÃ o link download â†’ Copy Link Address\n"
                    error_msg += "- Hoáº·c upload file CSV trá»±c tiáº¿p thay vÃ¬ dÃ¹ng URL"
                
                raise CSVError(error_msg)
            
            if 'text/csv' not in content_type and 'text/plain' not in content_type and 'application/octet-stream' not in content_type:
                logger.warning(f"Unexpected content type: {content_type}")
                raise CSVError(f"Content-Type khÃ´ng há»— trá»£: {content_type}\n\nâœ… **Cháº¥p nháº­n:** text/csv, text/plain, application/octet-stream\n\nðŸ’¡ URL cÃ³ thá»ƒ khÃ´ng trá» Ä‘áº¿n file CSV thá»±c sá»±.")
            
            # Check file size from headers
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > self.MAX_FILE_SIZE:
                raise CSVError(f"File quÃ¡ lá»›n ({int(content_length) / (1024*1024):.1f}MB). Max: {self.MAX_FILE_SIZE // (1024*1024)}MB")
            
            # Download with size limit
            content = b""
            for chunk in response.iter_content(chunk_size=8192):
                content += chunk
                if len(content) > self.MAX_FILE_SIZE:
                    raise CSVError(f"File quÃ¡ lá»›n (>{self.MAX_FILE_SIZE // (1024*1024)}MB). Vui lÃ²ng upload trá»±c tiáº¿p hoáº·c dÃ¹ng file nhá» hÆ¡n.")
            
            # Parse CSV
            try:
                df = pd.read_csv(StringIO(content.decode('utf-8')))
            except UnicodeDecodeError:
                # Try other encodings
                encodings = ['latin-1', 'iso-8859-1', 'cp1252']
                df = None
                for encoding in encodings:
                    try:
                        df = pd.read_csv(StringIO(content.decode(encoding)))
                        logger.info(f"Successfully decoded with {encoding}")
                        break
                    except Exception:
                        continue
                
                if df is None:
                    raise CSVError("KhÃ´ng thá»ƒ decode file CSV. Encoding khÃ´ng Ä‘Æ°á»£c há»— trá»£.\n\nðŸ’¡ Thá»­ lÆ°u file vá»›i UTF-8 encoding vÃ  upload trá»±c tiáº¿p.")
            
            # Validate and save
            df = self._validate_dataframe(df, csv_path)
            df.to_csv(csv_path, index=False)
            
            return df
            
        except requests.exceptions.Timeout:
            raise CSVError("â±ï¸ **Timeout khi táº£i file tá»« URL.**\n\nðŸ’¡ **Gá»£i Ã½:**\n- Thá»­ láº¡i sau\n- Upload file trá»±c tiáº¿p náº¿u file quÃ¡ lá»›n\n- Kiá»ƒm tra káº¿t ná»‘i máº¡ng")
        except requests.exceptions.ConnectionError:
            raise CSVError("ðŸ”Œ **KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n URL.**\n\nðŸ’¡ **Kiá»ƒm tra:**\n- URL cÃ³ Ä‘Ãºng khÃ´ng?\n- Káº¿t ná»‘i internet cÃ³ á»•n Ä‘á»‹nh?\n- Server cÃ³ Ä‘ang hoáº¡t Ä‘á»™ng?")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                raise CSVError("âŒ **404 Not Found:** File khÃ´ng tá»“n táº¡i.\n\nðŸ’¡ Kiá»ƒm tra láº¡i URL hoáº·c quyá»n truy cáº­p.")
            elif e.response.status_code == 403:
                raise CSVError("ðŸ”’ **403 Forbidden:** KhÃ´ng cÃ³ quyá»n truy cáº­p.\n\nðŸ’¡ File cÃ³ thá»ƒ lÃ  private hoáº·c cáº§n authentication.")
            else:
                raise CSVError(f"âŒ **HTTP {e.response.status_code}:** {str(e)}\n\nðŸ’¡ Thá»­ upload file trá»±c tiáº¿p.")
        except requests.exceptions.RequestException as e:
            raise CSVError(f"ðŸŒ **Lá»—i khi táº£i file tá»« URL:** {str(e)}\n\nðŸ’¡ Thá»­ upload file CSV trá»±c tiáº¿p thay vÃ¬ dÃ¹ng URL.")
        except pd.errors.EmptyDataError:
            raise CSVError("ðŸ“­ **File CSV tá»« URL khÃ´ng cÃ³ dá»¯ liá»‡u.**\n\nðŸ’¡ Kiá»ƒm tra láº¡i ná»™i dung file.")
        except pd.errors.ParserError as e:
            raise CSVError(f"âš ï¸ **Lá»—i parse CSV tá»« URL:** {str(e)}\n\nðŸ’¡ **CÃ³ thá»ƒ do:**\n- File khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng CSV\n- URL trá» Ä‘áº¿n trang HTML thay vÃ¬ file CSV\n- File bá»‹ há»ng\n\n**Giáº£i phÃ¡p:** Upload file trá»±c tiáº¿p Ä‘á»ƒ dá»… xá»­ lÃ½ hÆ¡n.")
        except Exception as e:
            logger.error(f"Unexpected error loading from URL: {e}")
            raise CSVError(f"âŒ **Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi táº£i tá»« URL:** {str(e)}\n\nðŸ’¡ **Khuyáº¿n nghá»‹:** Upload file CSV trá»±c tiáº¿p sáº½ á»•n Ä‘á»‹nh hÆ¡n.")

    def _validate_dataframe(self, df: pd.DataFrame, csv_path: Path) -> pd.DataFrame:
        """Validate DataFrame and apply constraints"""
        try:
            # Check if empty
            if df.empty:
                raise CSVError("DataFrame rá»—ng. File CSV khÃ´ng cÃ³ dá»¯ liá»‡u.")
            
            # Check number of rows
            if len(df) > self.MAX_ROWS:
                logger.warning(f"DataFrame has {len(df)} rows, truncating to {self.MAX_ROWS}")
                df = df.head(self.MAX_ROWS)
            
            # Check if has columns
            if len(df.columns) == 0:
                raise CSVError("File CSV khÃ´ng cÃ³ cá»™t nÃ o.")
            
            # Clean column names
            df.columns = df.columns.str.strip()
            
            # Check for all-null columns
            null_cols = df.columns[df.isnull().all()].tolist()
            if null_cols:
                logger.warning(f"Dropping all-null columns: {null_cols}")
                df = df.drop(columns=null_cols)
            
            # Log DataFrame info
            logger.info(f"DataFrame loaded successfully: {len(df)} rows, {len(df.columns)} columns")
            logger.info(f"Columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            if csv_path.exists():
                csv_path.unlink()
            raise CSVError(f"Lá»—i validate DataFrame: {str(e)}")

    async def query_dataframe(self, df: pd.DataFrame, query: str) -> Tuple[str, Optional[dict]]:
        """Query DataFrame with error handling"""
        chart_data = None
        
        try:
            # Check for common queries
            if "summarize" in query.lower() or "tÃ³m táº¯t" in query.lower():
                response = self.summarize_dataset(df)
            elif "stats" in query.lower() or "thá»‘ng kÃª" in query.lower():
                response = self.basic_stats(df)
            elif "missing" in query.lower() or "thiáº¿u" in query.lower():
                response = self.missing_values(df)
            elif "histogram" in query.lower() or "biá»ƒu Ä‘á»“" in query.lower():
                response, chart_data = self.create_histogram(df, query)
            else:
                # Use PandasQueryEngine for complex queries
                try:
                    query_engine = PandasQueryEngine(
                        df=df, 
                        verbose=True,
                        synthesize_response=True
                    )
                    result = query_engine.query(query)
                    response = str(result)
                except Exception as e:
                    logger.error(f"PandasQueryEngine error: {e}")
                    # Fallback to simple description
                    response = f"âš ï¸ KhÃ´ng thá»ƒ xá»­ lÃ½ cÃ¢u há»i phá»©c táº¡p.\n\n"
                    response += self.summarize_dataset(df)
                    response += "\n\nðŸ’¡ **Gá»£i Ã½:** Thá»­ cÃ¡c cÃ¢u há»i nhÆ°:\n"
                    response += "- `summarize` - TÃ³m táº¯t dataset\n"
                    response += "- `stats` - Thá»‘ng kÃª cÆ¡ báº£n\n"
                    response += "- `missing` - GiÃ¡ trá»‹ thiáº¿u\n"
                    response += f"- `histogram [tÃªn cá»™t]` - Biá»ƒu Ä‘á»“ (VÃ­ dá»¥: `histogram Age`)"
            
            return response, chart_data
            
        except Exception as e:
            logger.error(f"Error in query_dataframe: {e}")
            return f"âŒ Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}", None

    def summarize_dataset(self, df: pd.DataFrame) -> str:
        """Create a well-formatted dataset summary"""
        try:
            # Get data types info
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            summary = "## ðŸ“Š TÃ³m Táº¯t Dataset\n\n"
            
            # Basic info
            summary += "### ðŸ“ˆ ThÃ´ng tin cÆ¡ báº£n\n"
            summary += f"- **Tá»•ng sá»‘ dÃ²ng:** {len(df):,}\n"
            summary += f"- **Tá»•ng sá»‘ cá»™t:** {len(df.columns)}\n"
            summary += f"- **Cá»™t sá»‘:** {len(numeric_cols)}\n"
            summary += f"- **Cá»™t phÃ¢n loáº¡i:** {len(categorical_cols)}\n"
            summary += f"- **Dung lÆ°á»£ng bá»™ nhá»›:** {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n\n"
            
            # Column list with types
            summary += "### ðŸ“‹ Danh sÃ¡ch cá»™t\n"
            summary += "| # | TÃªn cá»™t | Kiá»ƒu dá»¯ liá»‡u | GiÃ¡ trá»‹ null |\n"
            summary += "|---|---------|-------------|-------------|\n"
            for idx, col in enumerate(df.columns, 1):
                dtype = str(df[col].dtype)
                null_count = df[col].isnull().sum()
                null_pct = f"{null_count/len(df)*100:.1f}%" if null_count > 0 else "0%"
                summary += f"| {idx} | `{col}` | {dtype} | {null_count:,} ({null_pct}) |\n"
            
            summary += "\n"
            
            # Preview data (formatted as table)
            summary += "### ðŸ‘€ Xem trÆ°á»›c dá»¯ liá»‡u (5 dÃ²ng Ä‘áº§u)\n\n"
            summary += self._format_dataframe_as_table(df.head())
            
            return summary
            
        except Exception as e:
            logger.error(f"Error in summarize_dataset: {e}")
            return f"âš ï¸ Lá»—i khi tÃ³m táº¯t: {str(e)}"

    def _format_dataframe_as_table(self, df: pd.DataFrame, max_rows: int = 10) -> str:
        """Format DataFrame as Markdown table with better formatting"""
        try:
            # Limit number of rows
            df_display = df.head(max_rows)
            
            # Create header
            table = "| " + " | ".join(df_display.columns) + " |\n"
            table += "|" + "|".join(["---"] * len(df_display.columns)) + "|\n"
            
            # Add rows
            for _, row in df_display.iterrows():
                formatted_row = []
                for val in row:
                    # Format values nicely
                    if pd.isna(val):
                        formatted_row.append("*null*")
                    elif isinstance(val, (int, float)):
                        if isinstance(val, float):
                            formatted_row.append(f"{val:,.2f}")
                        else:
                            formatted_row.append(f"{val:,}")
                    else:
                        # Truncate long strings
                        str_val = str(val)
                        if len(str_val) > 30:
                            str_val = str_val[:27] + "..."
                        formatted_row.append(str_val)
                
                table += "| " + " | ".join(formatted_row) + " |\n"
            
            return table
            
        except Exception as e:
            logger.error(f"Error formatting table: {e}")
            return f"```\n{df.to_string()}\n```"

    def basic_stats(self, df: pd.DataFrame) -> str:
        """Create well-formatted statistics summary"""
        try:
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
            
            if len(numeric_cols) == 0:
                return "âš ï¸ **KhÃ´ng cÃ³ cá»™t sá»‘ nÃ o** trong dataset Ä‘á»ƒ tÃ­nh thá»‘ng kÃª."
            
            stats = "## ðŸ“ˆ Thá»‘ng KÃª CÆ¡ Báº£n\n\n"
            
            for col in numeric_cols:
                stats += f"### ðŸ“Š {col}\n\n"
                
                col_data = df[col].dropna()
                
                stats += "| Chá»‰ sá»‘ | GiÃ¡ trá»‹ |\n"
                stats += "|--------|--------|\n"
                stats += f"| **Count** | {len(col_data):,} |\n"
                stats += f"| **Mean** | {col_data.mean():,.2f} |\n"
                stats += f"| **Std Dev** | {col_data.std():,.2f} |\n"
                stats += f"| **Min** | {col_data.min():,.2f} |\n"
                stats += f"| **25%** | {col_data.quantile(0.25):,.2f} |\n"
                stats += f"| **Median (50%)** | {col_data.median():,.2f} |\n"
                stats += f"| **75%** | {col_data.quantile(0.75):,.2f} |\n"
                stats += f"| **Max** | {col_data.max():,.2f} |\n"
                stats += f"| **Null values** | {df[col].isnull().sum():,} |\n\n"
            
            return stats
            
        except Exception as e:
            logger.error(f"Error in basic_stats: {e}")
            return f"âš ï¸ Lá»—i khi tÃ­nh thá»‘ng kÃª: {str(e)}"

    def missing_values(self, df: pd.DataFrame) -> str:
        """Create well-formatted missing values report"""
        try:
            missing = df.isnull().sum()
            missing = missing[missing > 0].sort_values(ascending=False)
            
            if len(missing) == 0:
                return "âœ… **KhÃ´ng cÃ³ giÃ¡ trá»‹ thiáº¿u** trong dataset!\n\nðŸŽ‰ Dá»¯ liá»‡u hoÃ n chá»‰nh vÃ  sáºµn sÃ ng Ä‘á»ƒ phÃ¢n tÃ­ch."
            
            result = "## âš ï¸ BÃ¡o CÃ¡o GiÃ¡ Trá»‹ Thiáº¿u\n\n"
            result += f"**Tá»•ng quan:** CÃ³ **{len(missing)}** cá»™t chá»©a giÃ¡ trá»‹ null.\n\n"
            
            result += "| TÃªn cá»™t | Sá»‘ null | Tá»· lá»‡ | Biá»ƒu Ä‘á»“ |\n"
            result += "|---------|---------|-------|--------|\n"
            
            for col, count in missing.items():
                pct = count / len(df) * 100
                # Create simple text progress bar
                bar_length = int(pct / 5)  # Scale to 20 chars max
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                result += f"| `{col}` | {count:,} | {pct:.1f}% | {bar} |\n"
            
            result += f"\n**ðŸ’¡ Gá»£i Ã½:** Xem xÃ©t xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u trÆ°á»›c khi phÃ¢n tÃ­ch sÃ¢u hÆ¡n."
            
            return result
            
        except Exception as e:
            logger.error(f"Error in missing_values: {e}")
            return f"âš ï¸ Lá»—i khi kiá»ƒm tra giÃ¡ trá»‹ thiáº¿u: {str(e)}"

    def create_histogram(self, df: pd.DataFrame, query: str) -> Tuple[str, Optional[dict]]:
        """Create histogram with better formatting"""
        try:
            # Extract column name from query
            words = query.lower().split()
            column = None
            for word in words:
                if word in [col.lower() for col in df.columns]:
                    column = [col for col in df.columns if col.lower() == word][0]
                    break
            
            if not column or column not in df.columns:
                avail_numeric = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
                return f"âš ï¸ **KhÃ´ng tÃ¬m tháº¥y cá»™t:** `{query}`\n\n**CÃ¡c cá»™t sá»‘ kháº£ dá»¥ng:**\n" + "\n".join([f"- `{col}`" for col in avail_numeric]), None
            
            if not pd.api.types.is_numeric_dtype(df[column]):
                return f"âš ï¸ Cá»™t `{column}` khÃ´ng pháº£i lÃ  sá»‘. KhÃ´ng thá»ƒ táº¡o histogram.", None
            
            # Create histogram data
            col_data = df[column].dropna()
            hist_data = col_data.value_counts().sort_index().head(50).to_dict()
            chart_data = {
                "type": "histogram",
                "column": column,
                "data": {str(k): int(v) for k, v in hist_data.items()}
            }
            
            response = f"## ðŸ“Š Histogram: `{column}`\n\n"
            
            # Statistics table
            response += "### ðŸ“ˆ Thá»‘ng kÃª\n\n"
            response += "| Chá»‰ sá»‘ | GiÃ¡ trá»‹ |\n"
            response += "|--------|--------|\n"
            response += f"| **Count** | {len(col_data):,} |\n"
            response += f"| **Min** | {col_data.min():,.2f} |\n"
            response += f"| **Max** | {col_data.max():,.2f} |\n"
            response += f"| **Mean** | {col_data.mean():,.2f} |\n"
            response += f"| **Median** | {col_data.median():,.2f} |\n"
            response += f"| **Std Dev** | {col_data.std():,.2f} |\n\n"
            
            # Distribution info
            response += "### ðŸ“‰ PhÃ¢n phá»‘i\n\n"
            response += f"Hiá»ƒn thá»‹ **{len(hist_data)}** giÃ¡ trá»‹ duy nháº¥t (unique values).\n\n"
            
            return response, chart_data
            
        except Exception as e:
            logger.error(f"Error in create_histogram: {e}")
            return f"âš ï¸ Lá»—i khi táº¡o histogram: {str(e)}", None

    async def get_history(self, conversation_id: str) -> List[Dict[str, Any]]:
        try:
            key = f"csv_chat_history:{conversation_id}"
            data = await redis_service.get(key)
            return json.loads(data) if data else []
        except Exception as e:
            logger.error(f"Error getting history: {e}")
            return []

    async def save_history(self, conversation_id: str, history: List[Dict[str, Any]]):
        try:
            key = f"csv_chat_history:{conversation_id}"
            await redis_service.set(key, json.dumps(history), expire=86400)
        except Exception as e:
            logger.error(f"Error saving history: {e}")

csv_service = CSVService()