from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime
import uuid
import json
import pandas as pd
from pathlib import Path
from llama_index.experimental.query_engine import PandasQueryEngine
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.core import Settings
from app.services.redis_service import redis_service
from app.core.config import OPENAI_API_KEY, OPENAI_MODEL_NAME
import logging
import requests
from io import StringIO

logger = logging.getLogger(__name__)

class CSVError(Exception):
    """Custom exception for CSV processing errors"""
    pass

class CSVService:
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    MAX_ROWS = 1_000_000  # 1 million rows
    ALLOWED_EXTENSIONS = {'.csv', '.txt'}
    
    def __init__(self):
        self.upload_dir = Path("uploads/csv")
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        
        # Configure llama-index to use gpt-4o-mini
        try:
            Settings.llm = LlamaOpenAI(
                api_key=OPENAI_API_KEY,
                model=OPENAI_MODEL_NAME,
                temperature=0.7
            )
            logger.info(f"LlamaIndex configured to use {OPENAI_MODEL_NAME}")
        except Exception as e:
            logger.error(f"Failed to configure LlamaIndex: {e}")

    async def process_csv_message(
        self,
        message: str,
        csv_data: Optional[bytes] = None,
        csv_url: Optional[str] = None,
        conversation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        conversation_id = conversation_id or str(uuid.uuid4())
        
        try:
            # **NEW: Check for conflicting inputs**
            if csv_data and csv_url:
                return {
                    "response": "‚ö†Ô∏è **L·ªói:** Kh√¥ng th·ªÉ x·ª≠ l√Ω ƒë·ªìng th·ªùi c·∫£ file upload v√† URL.\n\nüí° **G·ª£i √Ω:** Ch·ªâ ch·ªçn m·ªôt trong hai:\n- Upload file CSV tr·ª±c ti·∫øp, HO·∫∂C\n- Cung c·∫•p URL CSV",
                    "conversation_id": conversation_id
                }
        
            # Load or get DataFrame
            df = await self.get_or_load_dataframe(conversation_id, csv_data, csv_url)
            
            if df is None:
                return {
                    "response": "‚ö†Ô∏è **L·ªói:** Vui l√≤ng upload file CSV ho·∫∑c cung c·∫•p URL tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.\n\nüìÑ **ƒê·ªãnh d·∫°ng h·ªó tr·ª£:** .csv, .txt\nüìè **K√≠ch th∆∞·ªõc t·ªëi ƒëa:** 50MB",
                    "conversation_id": conversation_id
                }
            
            # Load history
            history = await self.get_history(conversation_id)
            
            # Add user message
            user_msg = {
                "role": "user",
                "content": message,
                "timestamp": datetime.now().isoformat()
            }
            history.append(user_msg)
            
            # Process query
            response_text, chart_data = await self.query_dataframe(df, message)
            
            # Add assistant message
            assistant_msg = {
                "role": "assistant",
                "content": response_text,
                "timestamp": datetime.now().isoformat()
            }
            history.append(assistant_msg)
            
            # Save history
            await self.save_history(conversation_id, history)
            
            return {
                "response": response_text,
                "conversation_id": conversation_id,
                "chart_data": chart_data
            }
            
        except CSVError as e:
            logger.error(f"CSV processing error: {e}")
            return {
                "response": f"‚ö†Ô∏è **L·ªói CSV:** {str(e)}",
                "conversation_id": conversation_id
            }
        except Exception as e:
            logger.error(f"Unexpected error in CSV processing: {e}")
            return {
                "response": "‚ùå **L·ªói kh√¥ng x√°c ƒë·ªãnh.** Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c upload file CSV kh√°c.",
                "conversation_id": conversation_id
            }

    async def get_or_load_dataframe(
        self,
        conversation_id: str,
        csv_data: Optional[bytes] = None,
        csv_url: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        csv_path = self.upload_dir / f"{conversation_id}.csv"
        
        # If CSV already exists for this conversation
        if csv_path.exists():
            try:
                return pd.read_csv(csv_path)
            except Exception as e:
                logger.error(f"Error reading existing CSV: {e}")
                raise CSVError("File CSV ƒë√£ l∆∞u b·ªã l·ªói. Vui l√≤ng upload l·∫°i.")
        
        # Load from bytes
        if csv_data:
            return await self._load_from_bytes(csv_data, csv_path)
        
        # Load from URL
        if csv_url:
            return await self._load_from_url(csv_url, csv_path)
        
        return None

    async def _load_from_bytes(self, csv_data: bytes, csv_path: Path) -> pd.DataFrame:
        """Load CSV from uploaded bytes with validation"""
        try:
            # Check file size
            if len(csv_data) > self.MAX_FILE_SIZE:
                raise CSVError(f"File qu√° l·ªõn (max {self.MAX_FILE_SIZE // (1024*1024)}MB). Vui l√≤ng upload file nh·ªè h∆°n.")
            
            # Check if empty
            if len(csv_data) == 0:
                raise CSVError("File CSV tr·ªëng. Vui l√≤ng upload file c√≥ d·ªØ li·ªáu.")
            
            # Save to temp file
            with open(csv_path, "wb") as f:
                f.write(csv_data)
            
            # Try to read with multiple encodings
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            df = None
            last_error = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding)
                    break
                except UnicodeDecodeError as e:
                    last_error = e
                    continue
            
            if df is None:
                raise CSVError(f"Kh√¥ng th·ªÉ ƒë·ªçc file CSV. Encoding kh√¥ng h·ª£p l·ªá: {last_error}")
            
            # Validate DataFrame
            return self._validate_dataframe(df, csv_path)
            
        except pd.errors.EmptyDataError:
            raise CSVError("File CSV kh√¥ng c√≥ d·ªØ li·ªáu.")
        except pd.errors.ParserError as e:
            raise CSVError(f"L·ªói parse CSV: {str(e)}. File c√≥ th·ªÉ b·ªã h·ªèng ho·∫∑c kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng.")
        except Exception as e:
            if csv_path.exists():
                csv_path.unlink()  # Clean up
            raise CSVError(f"L·ªói khi ƒë·ªçc file: {str(e)}")

    async def _load_from_url(self, csv_url: str, csv_path: Path) -> Optional[pd.DataFrame]:
        """Load CSV from URL with validation"""
        try:
            # Validate URL format
            if not csv_url.startswith(('http://', 'https://')):
                raise CSVError("URL kh√¥ng h·ª£p l·ªá. URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
            
            # **NEW: Auto-convert GitHub URLs to raw URLs**
            if 'github.com' in csv_url and '/blob/' in csv_url:
                csv_url = csv_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                logger.info(f"Converted GitHub URL to raw URL: {csv_url}")
            
            # Check if URL ends with .csv
            if not any(csv_url.lower().endswith(ext) for ext in ['.csv', '.txt']):
                raise CSVError("URL ph·∫£i k·∫øt th√∫c b·∫±ng .csv ho·∫∑c .txt\n\nüí° **V√≠ d·ª•:** https://example.com/data.csv")
            # Try to read with timeout and size limit
            from io import StringIO

            logger.info(f"Fetching CSV from URL: {csv_url}")
            
            response = requests.get(csv_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '').lower()
            
            # **NEW: Better content-type validation with helpful error**
            if 'text/html' in content_type:
                logger.warning(f"Received HTML instead of CSV from: {csv_url}")
                error_msg = "‚ö†Ô∏è **URL tr·∫£ v·ªÅ trang HTML thay v√¨ file CSV.**\n\n"
                
                if 'github.com' in csv_url:
                    raw_url = csv_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                    error_msg += f"üí° **GitHub URL:** S·ª≠ d·ª•ng URL raw thay v√¨ URL blob:\n\n"
                    error_msg += f"‚ùå **Sai:** `{csv_url}`\n\n"
                    error_msg += f"‚úÖ **ƒê√∫ng:** `{raw_url}`\n\n"
                    error_msg += f"**Ho·∫∑c:** Click n√∫t 'Raw' tr√™n trang GitHub ƒë·ªÉ l·∫•y URL ƒë√∫ng."
                else:
                    error_msg += "üí° **G·ª£i √Ω:**\n"
                    error_msg += "- ƒê·∫£m b·∫£o URL tr·ªè tr·ª±c ti·∫øp ƒë·∫øn file CSV (kh√¥ng ph·∫£i trang web)\n"
                    error_msg += "- Th·ª≠ click chu·ªôt ph·∫£i v√†o link download ‚Üí Copy Link Address\n"
                    error_msg += "- Ho·∫∑c upload file CSV tr·ª±c ti·∫øp thay v√¨ d√πng URL"
                
                raise CSVError(error_msg)
            
            if 'text/csv' not in content_type and 'text/plain' not in content_type and 'application/octet-stream' not in content_type:
                logger.warning(f"Unexpected content type: {content_type}")
                raise CSVError(f"Content-Type kh√¥ng h·ªó tr·ª£: {content_type}\n\n‚úÖ **Ch·∫•p nh·∫≠n:** text/csv, text/plain, application/octet-stream\n\nüí° URL c√≥ th·ªÉ kh√¥ng tr·ªè ƒë·∫øn file CSV th·ª±c s·ª±.")
            
            # Check file size from headers
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > self.MAX_FILE_SIZE:
                raise CSVError(f"File qu√° l·ªõn ({int(content_length) / (1024*1024):.1f}MB). Max: {self.MAX_FILE_SIZE // (1024*1024)}MB")
            
            # Download with size limit
            content = b""
            for chunk in response.iter_content(chunk_size=8192):
                content += chunk
                if len(content) > self.MAX_FILE_SIZE:
                    raise CSVError(f"File qu√° l·ªõn (>{self.MAX_FILE_SIZE // (1024*1024)}MB). Vui l√≤ng upload tr·ª±c ti·∫øp ho·∫∑c d√πng file nh·ªè h∆°n.")
            
            # Parse CSV
            try:
                df = pd.read_csv(StringIO(content.decode('utf-8')))
            except UnicodeDecodeError:
                # Try other encodings
                encodings = ['latin-1', 'iso-8859-1', 'cp1252']
                df = None
                for encoding in encodings:
                    try:
                        df = pd.read_csv(StringIO(content.decode(encoding)))
                        logger.info(f"Successfully decoded with {encoding}")
                        break
                    except Exception:
                        continue
                
                if df is None:
                    raise CSVError("Kh√¥ng th·ªÉ decode file CSV. Encoding kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.\n\nüí° Th·ª≠ l∆∞u file v·ªõi UTF-8 encoding v√† upload tr·ª±c ti·∫øp.")
            
            # Validate and save
            df = self._validate_dataframe(df, csv_path)
            df.to_csv(csv_path, index=False)
            
            return df
            
        except requests.exceptions.Timeout:
            raise CSVError("‚è±Ô∏è **Timeout khi t·∫£i file t·ª´ URL.**\n\nüí° **G·ª£i √Ω:**\n- Th·ª≠ l·∫°i sau\n- Upload file tr·ª±c ti·∫øp n·∫øu file qu√° l·ªõn\n- Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng")
        except requests.exceptions.ConnectionError:
            raise CSVError("üîå **Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn URL.**\n\nüí° **Ki·ªÉm tra:**\n- URL c√≥ ƒë√∫ng kh√¥ng?\n- K·∫øt n·ªëi internet c√≥ ·ªïn ƒë·ªãnh?\n- Server c√≥ ƒëang ho·∫°t ƒë·ªông?")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                raise CSVError("‚ùå **404 Not Found:** File kh√¥ng t·ªìn t·∫°i.\n\nüí° Ki·ªÉm tra l·∫°i URL ho·∫∑c quy·ªÅn truy c·∫≠p.")
            elif e.response.status_code == 403:
                raise CSVError("üîí **403 Forbidden:** Kh√¥ng c√≥ quy·ªÅn truy c·∫≠p.\n\nüí° File c√≥ th·ªÉ l√† private ho·∫∑c c·∫ßn authentication.")
            else:
                raise CSVError(f"‚ùå **HTTP {e.response.status_code}:** {str(e)}\n\nüí° Th·ª≠ upload file tr·ª±c ti·∫øp.")
        except requests.exceptions.RequestException as e:
            raise CSVError(f"üåê **L·ªói khi t·∫£i file t·ª´ URL:** {str(e)}\n\nüí° Th·ª≠ upload file CSV tr·ª±c ti·∫øp thay v√¨ d√πng URL.")
        except pd.errors.EmptyDataError:
            raise CSVError("üì≠ **File CSV t·ª´ URL kh√¥ng c√≥ d·ªØ li·ªáu.**\n\nüí° Ki·ªÉm tra l·∫°i n·ªôi dung file.")
        except pd.errors.ParserError as e:
            raise CSVError(f"‚ö†Ô∏è **L·ªói parse CSV t·ª´ URL:** {str(e)}\n\nüí° **C√≥ th·ªÉ do:**\n- File kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng CSV\n- URL tr·ªè ƒë·∫øn trang HTML thay v√¨ file CSV\n- File b·ªã h·ªèng\n\n**Gi·∫£i ph√°p:** Upload file tr·ª±c ti·∫øp ƒë·ªÉ d·ªÖ x·ª≠ l√Ω h∆°n.")
        except Exception as e:
            logger.error(f"Unexpected error loading from URL: {e}")
            raise CSVError(f"‚ùå **L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i t·ª´ URL:** {str(e)}\n\nüí° **Khuy·∫øn ngh·ªã:** Upload file CSV tr·ª±c ti·∫øp s·∫Ω ·ªïn ƒë·ªãnh h∆°n.")

    def _validate_dataframe(self, df: pd.DataFrame, csv_path: Path) -> pd.DataFrame:
        """Validate DataFrame and apply constraints"""
        try:
            # Check if empty
            if df.empty:
                raise CSVError("DataFrame r·ªóng. File CSV kh√¥ng c√≥ d·ªØ li·ªáu.")
            
            # Check number of rows
            if len(df) > self.MAX_ROWS:
                logger.warning(f"DataFrame has {len(df)} rows, truncating to {self.MAX_ROWS}")
                df = df.head(self.MAX_ROWS)
            
            # Check if has columns
            if len(df.columns) == 0:
                raise CSVError("File CSV kh√¥ng c√≥ c·ªôt n√†o.")
            
            # Clean column names
            df.columns = df.columns.str.strip()
            
            # Check for all-null columns
            null_cols = df.columns[df.isnull().all()].tolist()
            if null_cols:
                logger.warning(f"Dropping all-null columns: {null_cols}")
                df = df.drop(columns=null_cols)
            
            # Log DataFrame info
            logger.info(f"DataFrame loaded successfully: {len(df)} rows, {len(df.columns)} columns")
            logger.info(f"Columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            if csv_path.exists():
                csv_path.unlink()
            raise CSVError(f"L·ªói validate DataFrame: {str(e)}")

    async def query_dataframe(self, df: pd.DataFrame, query: str) -> Tuple[str, Optional[dict]]:
        """Query DataFrame with error handling"""
        chart_data = None
        
        try:
            # Check for common queries
            if "summarize" in query.lower() or "t√≥m t·∫Øt" in query.lower():
                response = self.summarize_dataset(df)
            elif "stats" in query.lower() or "th·ªëng k√™" in query.lower():
                response = self.basic_stats(df)
            elif "missing" in query.lower() or "thi·∫øu" in query.lower():
                response = self.missing_values(df)
            elif "histogram" in query.lower() or "bi·ªÉu ƒë·ªì" in query.lower():
                response, chart_data = self.create_histogram(df, query)
            else:
                # Use PandasQueryEngine for complex queries
                try:
                    query_engine = PandasQueryEngine(
                        df=df, 
                        verbose=True,
                        synthesize_response=True
                    )
                    result = query_engine.query(query)
                    response = str(result)
                except Exception as e:
                    logger.error(f"PandasQueryEngine error: {e}")
                    # Fallback to simple description
                    response = f"‚ö†Ô∏è Kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p.\n\n"
                    response += self.summarize_dataset(df)
                    response += "\n\nüí° **G·ª£i √Ω:** Th·ª≠ c√°c c√¢u h·ªèi nh∆∞:\n"
                    response += "- `summarize` - T√≥m t·∫Øt dataset\n"
                    response += "- `stats` - Th·ªëng k√™ c∆° b·∫£n\n"
                    response += "- `missing` - Gi√° tr·ªã thi·∫øu\n"
                    response += f"- `histogram [t√™n c·ªôt]` - Bi·ªÉu ƒë·ªì (V√≠ d·ª•: `histogram Age`)"
            
            return response, chart_data
            
        except Exception as e:
            logger.error(f"Error in query_dataframe: {e}")
            return f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}", None

    def summarize_dataset(self, df: pd.DataFrame) -> str:
        """Create a well-formatted dataset summary"""
        try:
            # Get data types info
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            summary = "## üìä T√≥m T·∫Øt Dataset\n\n"
            
            # Basic info
            summary += "### üìà Th√¥ng tin c∆° b·∫£n\n"
            summary += f"- **T·ªïng s·ªë d√≤ng:** {len(df):,}\n"
            summary += f"- **T·ªïng s·ªë c·ªôt:** {len(df.columns)}\n"
            summary += f"- **C·ªôt s·ªë:** {len(numeric_cols)}\n"
            summary += f"- **C·ªôt ph√¢n lo·∫°i:** {len(categorical_cols)}\n"
            summary += f"- **Dung l∆∞·ª£ng b·ªô nh·ªõ:** {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n\n"
            
            # Column list with types
            summary += "### üìã Danh s√°ch c·ªôt\n"
            summary += "| # | T√™n c·ªôt | Ki·ªÉu d·ªØ li·ªáu | Gi√° tr·ªã null |\n"
            summary += "|---|---------|-------------|-------------|\n"
            for idx, col in enumerate(df.columns, 1):
                dtype = str(df[col].dtype)
                null_count = df[col].isnull().sum()
                null_pct = f"{null_count/len(df)*100:.1f}%" if null_count > 0 else "0%"
                summary += f"| {idx} | `{col}` | {dtype} | {null_count:,} ({null_pct}) |\n"
            
            summary += "\n"
            
            # Preview data (formatted as table)
            summary += "### üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (5 d√≤ng ƒë·∫ßu)\n\n"
            summary += self._format_dataframe_as_table(df.head())
            
            return summary
            
        except Exception as e:
            logger.error(f"Error in summarize_dataset: {e}")
            return f"‚ö†Ô∏è L·ªói khi t√≥m t·∫Øt: {str(e)}"

    def _format_dataframe_as_table(self, df: pd.DataFrame, max_rows: int = 10) -> str:
        """Format DataFrame as Markdown table with better formatting"""
        try:
            # Limit number of rows
            df_display = df.head(max_rows)
            
            # Create header
            table = "| " + " | ".join(df_display.columns) + " |\n"
            table += "|" + "|".join(["---"] * len(df_display.columns)) + "|\n"
            
            # Add rows
            for _, row in df_display.iterrows():
                formatted_row = []
                for val in row:
                    # Format values nicely
                    if pd.isna(val):
                        formatted_row.append("*null*")
                    elif isinstance(val, (int, float)):
                        if isinstance(val, float):
                            formatted_row.append(f"{val:,.2f}")
                        else:
                            formatted_row.append(f"{val:,}")
                    else:
                        # Truncate long strings
                        str_val = str(val)
                        if len(str_val) > 30:
                            str_val = str_val[:27] + "..."
                        formatted_row.append(str_val)
                
                table += "| " + " | ".join(formatted_row) + " |\n"
            
            return table
            
        except Exception as e:
            logger.error(f"Error formatting table: {e}")
            return f"```\n{df.to_string()}\n```"

    def basic_stats(self, df: pd.DataFrame) -> str:
        """Create well-formatted statistics summary"""
        try:
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
            
            if len(numeric_cols) == 0:
                return "‚ö†Ô∏è **Kh√¥ng c√≥ c·ªôt s·ªë n√†o** trong dataset ƒë·ªÉ t√≠nh th·ªëng k√™."
            
            stats = "## üìà Th·ªëng K√™ C∆° B·∫£n\n\n"
            
            for col in numeric_cols:
                stats += f"### üìä {col}\n\n"
                
                col_data = df[col].dropna()
                
                stats += "| Ch·ªâ s·ªë | Gi√° tr·ªã |\n"
                stats += "|--------|--------|\n"
                stats += f"| **Count** | {len(col_data):,} |\n"
                stats += f"| **Mean** | {col_data.mean():,.2f} |\n"
                stats += f"| **Std Dev** | {col_data.std():,.2f} |\n"
                stats += f"| **Min** | {col_data.min():,.2f} |\n"
                stats += f"| **25%** | {col_data.quantile(0.25):,.2f} |\n"
                stats += f"| **Median (50%)** | {col_data.median():,.2f} |\n"
                stats += f"| **75%** | {col_data.quantile(0.75):,.2f} |\n"
                stats += f"| **Max** | {col_data.max():,.2f} |\n"
                stats += f"| **Null values** | {df[col].isnull().sum():,} |\n\n"
            
            return stats
            
        except Exception as e:
            logger.error(f"Error in basic_stats: {e}")
            return f"‚ö†Ô∏è L·ªói khi t√≠nh th·ªëng k√™: {str(e)}"

    def missing_values(self, df: pd.DataFrame) -> str:
        """Create well-formatted missing values report"""
        try:
            missing = df.isnull().sum()
            missing = missing[missing > 0].sort_values(ascending=False)
            
            if len(missing) == 0:
                return "‚úÖ **Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu** trong dataset!\n\nüéâ D·ªØ li·ªáu ho√†n ch·ªânh v√† s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch."
            
            result = "## ‚ö†Ô∏è B√°o C√°o Gi√° Tr·ªã Thi·∫øu\n\n"
            result += f"**T·ªïng quan:** C√≥ **{len(missing)}** c·ªôt ch·ª©a gi√° tr·ªã null.\n\n"
            
            result += "| T√™n c·ªôt | S·ªë null | T·ª∑ l·ªá | Bi·ªÉu ƒë·ªì |\n"
            result += "|---------|---------|-------|--------|\n"
            
            for col, count in missing.items():
                pct = count / len(df) * 100
                # Create simple text progress bar
                bar_length = int(pct / 5)  # Scale to 20 chars max
                bar = "‚ñà" * bar_length + "‚ñë" * (20 - bar_length)
                result += f"| `{col}` | {count:,} | {pct:.1f}% | {bar} |\n"
            
            result += f"\n**üí° G·ª£i √Ω:** Xem x√©t x·ª≠ l√Ω gi√° tr·ªã thi·∫øu tr∆∞·ªõc khi ph√¢n t√≠ch s√¢u h∆°n."
            
            return result
            
        except Exception as e:
            logger.error(f"Error in missing_values: {e}")
            return f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra gi√° tr·ªã thi·∫øu: {str(e)}"

    def create_histogram(self, df: pd.DataFrame, query: str) -> Tuple[str, Optional[dict]]:
        """Create histogram with better formatting"""
        try:
            # Extract column name from query
            words = query.lower().split()
            column = None
            for word in words:
                if word in [col.lower() for col in df.columns]:
                    column = [col for col in df.columns if col.lower() == word][0]
                    break
            
            if not column or column not in df.columns:
                avail_numeric = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
                return f"‚ö†Ô∏è **Kh√¥ng t√¨m th·∫•y c·ªôt:** `{query}`\n\n**C√°c c·ªôt s·ªë kh·∫£ d·ª•ng:**\n" + "\n".join([f"- `{col}`" for col in avail_numeric]), None
            
            if not pd.api.types.is_numeric_dtype(df[column]):
                return f"‚ö†Ô∏è C·ªôt `{column}` kh√¥ng ph·∫£i l√† s·ªë. Kh√¥ng th·ªÉ t·∫°o histogram.", None
            
            # Create histogram data
            col_data = df[column].dropna()
            hist_data = col_data.value_counts().sort_index().head(50).to_dict()
            chart_data = {
                "type": "histogram",
                "column": column,
                "data": {str(k): int(v) for k, v in hist_data.items()}
            }
            
            response = f"## üìä Histogram: `{column}`\n\n"
            
            # Statistics table
            response += "### üìà Th·ªëng k√™\n\n"
            response += "| Ch·ªâ s·ªë | Gi√° tr·ªã |\n"
            response += "|--------|--------|\n"
            response += f"| **Count** | {len(col_data):,} |\n"
            response += f"| **Min** | {col_data.min():,.2f} |\n"
            response += f"| **Max** | {col_data.max():,.2f} |\n"
            response += f"| **Mean** | {col_data.mean():,.2f} |\n"
            response += f"| **Median** | {col_data.median():,.2f} |\n"
            response += f"| **Std Dev** | {col_data.std():,.2f} |\n\n"
            
            # Distribution info
            response += "### üìâ Ph√¢n ph·ªëi\n\n"
            response += f"Hi·ªÉn th·ªã **{len(hist_data)}** gi√° tr·ªã duy nh·∫•t (unique values).\n\n"
            
            return response, chart_data
            
        except Exception as e:
            logger.error(f"Error in create_histogram: {e}")
            return f"‚ö†Ô∏è L·ªói khi t·∫°o histogram: {str(e)}", None

    async def get_history(self, conversation_id: str) -> List[Dict[str, Any]]:
        try:
            key = f"csv_chat_history:{conversation_id}"
            data = await redis_service.get(key)
            return json.loads(data) if data else []
        except Exception as e:
            logger.error(f"Error getting history: {e}")
            return []

    async def save_history(self, conversation_id: str, history: List[Dict[str, Any]]):
        try:
            key = f"csv_chat_history:{conversation_id}"
            await redis_service.set(key, json.dumps(history), expire=86400)
        except Exception as e:
            logger.error(f"Error saving history: {e}")

csv_service = CSVService()